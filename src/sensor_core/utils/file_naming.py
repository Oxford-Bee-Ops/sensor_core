from datetime import datetime
from pathlib import Path
from random import random
from typing import NamedTuple, Optional

from sensor_core import api
from sensor_core import configuration as root_cfg

logger = root_cfg.setup_logger("sensor_core")

class DATA_ID(NamedTuple):
    """Datastream ID is composed of {datastream_type_id}_{device_id}_{sensor_id}"""
    type_id: str
    device_id: str
    sensor_index: int
    stream_index: int

    def __str__(self) -> str:
        return f"{self.type_id}_{self.device_id}_{self.sensor_index:02d}_{self.stream_index:02d}"


def create_data_id(device_id: str, 
                   sensor_index: int,
                   type_id: str, 
                   stream_index: int) -> str:
    """Returns a standard, universally unique, identifier for the Datastream"""
    return f"{type_id}_{device_id}_{sensor_index:02d}_{stream_index:02d}"

def parse_data_id(data_id: str) -> DATA_ID:
    """Parse a Datastream ID into its components.

    Parameters
    ----------
    data_id - Datastream ID to be parsed

    Return
    ------
    A DATA_ID NamedTuple with the following components:
        bapi.RECORD_ID.DS_TYPE_ID       - always
        bapi.RECORD_ID.DEVICE_ID        - always
        bapi.RECORD_ID.SENSOR_INDEX     - always
        bapi.RECORD_ID.STREAM_INDEX     - always
    """
    fields = data_id.split("_")
    assert len(fields) == 4, f"Error parsing data_id:{data_id}"
    return DATA_ID(
        type_id=fields[0],
        device_id=fields[1],
        sensor_index=int(fields[2]),
        stream_index=int(fields[3]),
    )
        

def parse_record_filename(fname: Path | str) -> dict:
    """Parse a filename generated by a Datastream object to extract all of the fields.

    Parameters
    ----------
    fname - Path or string containing the filename to be parse

    Return
    ------
    A dictionary with the following components derived from the filename:
        bapi.RECORD_ID.VERSION          - always
        bapi.RECORD_ID.DS_TYPE_ID       - always
        bapi.RECORD_ID.DEVICE_ID        - always
        bapi.RECORD_ID.SENSOR_INDEX     - always
        bapi.RECORD_ID.STREAM_INDEX     - always
        bapi.RECORD_ID.TIMESTAMP        - always
        bapi.RECORD_ID.SUFFIX           - always
        bapi.RECORD_ID.END_TIME         - if present
        bapi.RECORD_ID.OFFSET           - if present
        bapi.RECORD_ID.SECONDARY_OFFSET - if present
        bapi.RECORD_ID.INCREMENT        - always
    """

    logger.debug(f"Parsing filename: {fname}")
    if isinstance(fname, str):
        fname = Path(fname)

    # Check that the filename has at least 5 "_"
    if fname.name.count("_") < 5:
        logger.warning(f"Invalid filename format - too few _ in {fname.name}")
        return {}

    stem = fname.stem

    # First remove any increment counter used to create a unique filename
    # This is a double underscore, so we can split on it and take the first part
    increment = 0
    if "__" in stem:
        stem = stem.split("__")[0]
        increment = int(stem.split("__")[1])

    # Extract the fields from the filename, parsing with the "_" delimiter
    fields = stem.split("_")
    end_time = None
    primary_offset_index = None
    secondary_offset_index = None
    # Version is the first field, 2 characters like 'V3'
    version = fields[0]
    datastream_type_id = fields[1]
    # Re-expand the device_id (field 0) to include the underscores after each 2 characters
    device_id = fields[2]
    if len(device_id) != 12:
        raise ValueError(f"Error parsing filename device_id:{fname}")
    sensor_id = int(fields[3])
    stream_index = int(fields[4])
    # Convert the start_time and end_time to datetime objects
    start_time = api.utc_from_str(fields[5])
    if len(fields) > 6:
        end_time = api.utc_from_str(fields[6])
    if len(fields) > 7:
        primary_offset_index = int(fields[7])
    if len(fields) > 8:
        secondary_offset_index = int(fields[8])

    fields_dict = {
        api.RECORD_ID.VERSION.value: version,
        api.RECORD_ID.DEVICE_ID.value: device_id,
        api.RECORD_ID.SENSOR_INDEX.value: sensor_id,
        api.RECORD_ID.STREAM_INDEX.value: stream_index,
        api.RECORD_ID.DATA_TYPE_ID.value: datastream_type_id,
        api.RECORD_ID.TIMESTAMP.value: start_time,
        api.RECORD_ID.END_TIME.value: end_time,
        api.RECORD_ID.OFFSET.value: primary_offset_index,
        api.RECORD_ID.SECONDARY_OFFSET.value: secondary_offset_index,
        api.RECORD_ID.SUFFIX.value: fname.suffix[1:],
        api.RECORD_ID.INCREMENT.value: increment,
    }
    logger.debug(f"Parsed fname {fname} to {fields_dict}")
    return fields_dict

def get_file_datetime(fname: Path | str) -> datetime:
    """Get the UTC timestamp from the filename."""
    if isinstance(fname, str):
        fname = Path(fname)

    fname = fname.stem

    # Check that the filename has at least 4 "_"
    if fname.count("_") < 4:
        logger.warning(f"Invalid filename format - too few _ in {fname}")
        return datetime.min

    # Extract the fields from the filename, parsing with the "_" delimiter
    fields = fname.split("_")
    start_time = api.utc_from_str(fields[4])
    return start_time
    

def get_record_filename(
    dst_dir: Path,
    data_id: str,
    suffix: api.FORMAT,
    start_time: datetime,
    end_time: Optional[datetime] = None,
    frame_number: Optional[int] = None,
    arbitrary_index: Optional[int] = None,
)->Path:
    """Generate the filename for the recording file.

    datastream_id is composed of {datastream_type_id}_{device_id}_{sensor_id}

    The V3 filename format is one of:
        V3_{datastream_type_id}_{device_id}_{sensor_id}_{start_time}.{suffix}
        V3_{datastream_type_id}_{device_id}_{sensor_id}_{start_time}_{end_time}.{suffix}
        V3_{datastream_type_id}_{device_id}_{sensor_id}_{start_time}_{end_time}_{frame_number}.{suffix}
        V3_{datastream_type_id}_{device_id}_{sensor_id}_{start_time}_{end_time}_{frame_number}_
            {arbitrary_index}.{suffix} 

    Fieldsrare separated by "_" and the following are fixed width.
        - datastream_type_id: 5 characters
        - device_id: 12 characters mac address
        - sensor_id: 2 characters, 0 padded as necessary
        - start_time including milliseconds: 17 characters
        - end_time including milliseconds: 17 characters
    """
    if end_time is None:
        assert frame_number is None, "Frame number is only valid if an end_time is specified."
        assert arbitrary_index is None, "Arbitrary index is only valid if an end_time is specified."
    elif frame_number is None:
        assert arbitrary_index is None, "Arbitrary index is only valid if a frame number is specified."

    fname = f"V3_{data_id}_{api.utc_to_fname_str(start_time)}"
    if end_time is not None:
        fname += f"_{api.utc_to_fname_str(end_time)}"
        if frame_number is not None:
            fname += f"_{frame_number}"
            if arbitrary_index is not None:
                fname += f"_{arbitrary_index}"

    # The suffix can be explicitly passed in, or defaults to the ds_type.input_format
    fname += f".{suffix.value}"

    # Add the path to the specified edge processing directory, creating the directory if it does not exist
    assert dst_dir is not None
    assert isinstance(dst_dir, Path)
    if not dst_dir.exists():
        dst_dir.mkdir(parents=True, exist_ok=True)
    full_fname = dst_dir / fname

    return full_fname


def increment_filename(fname: Path) -> Path:
    """If a filename exists, increment the filename to avoid overwriting existing files."""
    new_fname = fname
    count = 1
    while new_fname.exists():
        new_fname = fname.with_name(f"{fname.stem}__{count}{fname.suffix}")
        count += 1
        if count > 100:
            raise Exception(f"Error incrementing filename {fname}, count > 100")
    return new_fname


def get_cloud_journal_filename(type_id: str, day: datetime) -> Path:
    """Filenaming for cloud journals based on date."""
    if root_cfg.get_mode() == root_cfg.Mode.EDGE:
        processing_dir = root_cfg.EDGE_PROCESSING_DIR
    else:
        processing_dir = root_cfg.ETL_PROCESSING_DIR

    return processing_dir.joinpath(
            f"V3_{type_id}_{root_cfg.my_device_id}_{day.strftime('%Y%m%d')}.csv"
        )


def get_journal_filename(type_id: str) -> Path:
    """Filenaming for live journals that are in use; they get renamed with a timestamp when closed."""
    return root_cfg.EDGE_STAGING_DIR.joinpath(f"V3_{type_id}_{root_cfg.my_device_id}.csv")


def get_temporary_filename(format: api.FORMAT) -> Path:
    """Generate a temporary filename in the TMP_DIR with the specified suffix."""
    suffix = format.value
    tmp_fname = root_cfg.TMP_DIR.joinpath(f"tmp_{api.utc_to_fname_str()}_{random():.4g}.{suffix}")
    return tmp_fname


def get_zip_filename() -> Path:
    """Generate a filename for a zip file in the upload directory."""
    return root_cfg.EDGE_UPLOAD_DIR / f"V3_{root_cfg.my_device_id}_{api.utc_to_fname_str()}.zip"


def get_log_filename() -> Path:
    """Generate a filename for a log file in the upload directory."""
    return root_cfg.EDGE_UPLOAD_DIR / f"V3_{root_cfg.my_device_id}_{api.utc_to_fname_str()}.log"

def get_FAIR_filename(sensor_type: api.SENSOR_TYPE, sensor_index: int, suffix: str) -> Path:
    """Generate a filename for a fair file."""
    return (
        root_cfg.EDGE_UPLOAD_DIR / 
        f"V3_{root_cfg.my_device_id}_{sensor_type.value}_{sensor_index}_"
        f"{api.utc_to_fname_str()}.{suffix}"
    )